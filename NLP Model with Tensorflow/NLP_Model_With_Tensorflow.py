# -*- coding: utf-8 -*-
"""NLP-Submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e1eMaoxI06Redyj0yRK-THAOL4j3f9Pq

# **Membuat Model NLP dengan TensorFlow**
- Nama: Muhammad Hafiz
- Email: mhdhfz391@gmail.com
- Id Dicoding: mhdhfzz

## **Menyiapkan semua library yang dibutuhkan**
"""

#import library
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords, wordnet
from nltk import pos_tag
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from string import punctuation
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import nltk
import re
import string
import unicodedata

nltk.download('stopwords')

"""## **Membaca dataset dan visualisasi data berdasarkan Theme**"""

!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=19FPrDU0nT0SpGBKlwHYCx88I5OeyU3YQ' -O CNN_Articels_clean.csv

#import data ke variabel untuk dibaca
news_data = pd.read_csv('CNN_Articels_clean.csv')

#cek 5 data teratas
news_data.head()

#hapus columns yang tidak digunakan
del_col = ['Index','Author','Date published','Section','Url','Keywords','Second headline','Article text']
news_data = news_data.drop(columns=del_col)
news_data

#total data
news_data.shape

news_data.info()

#Category
news_data.Category.value_counts()

# Cek apakah ada nilai Null
news_data.isnull().sum()

#visualisasi data berdasarkan Category
sns.set(style="darkgrid")
plt.figure(figsize=(12, 6))
sns.countplot(x="Category", data=news_data, order=news_data['Category'].value_counts().index)
plt.xticks(rotation=45, ha='right')
plt.xlabel('Category')
plt.ylabel('Count')
plt.show()

#menggabungkan column Headline dan Description
news_data['Text'] = news_data['Headline'] + " " + news_data['Description']
news_data

"""## **Data Cleaning**

### **Menghapus Stopwords**

Stop words adalah kata umum (common words) yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna. Contoh stop words untuk bahasa Inggris diantaranya “of”, “i”, dll. Kata tersebut sudah ada dalah corpus
"""

stwd = set(stopwords.words('english'))
punctuation = list(string.punctuation)
stwd.update(punctuation)

# data cleaning
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

#hapus square brackets
def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)
#hapus URL's
def remove_url(text):
    return re.sub(r'http\S+', '', text)
#hapus stopwords dari text
def remove_stopwords(text):
    final_text = []
    for i in text.split():
        if i.strip().lower() not in stwd:
            final_text.append(i.strip())
    return " ".join(final_text)
#hapus noisy text
def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    text = remove_url(text)
    text = remove_stopwords(text)
    return text
#Apply function on review column
news_data['Text']=news_data['Text'].apply(denoise_text)

"""### **Jumlah Word**"""

def get_corpus(text):
    words = []
    for i in text:
        for j in i.split():
            words.append(j.strip())
    return words
corpus = get_corpus(news_data.Text)
corpus[:10]

#jumlah kata yang sering ditemukan
from collections import Counter
counter = Counter(corpus)
most_common = counter.most_common(10)
most_common = dict(most_common)
most_common

def get_top_text_ngrams(corpus, n, g):
    cv = CountVectorizer(ngram_range=(g, g)).fit(corpus)
    bag_words = cv.transform(corpus)
    sum_words = bag_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

plt.figure(figsize = (16,9))
most_common = get_top_text_ngrams(news_data.Text,10,1)
most_common = dict(most_common)
sns.barplot(x=list(most_common.values()),y=list(most_common.keys()))

"""### **Encoding & Splitting Data**"""

# data Category one-hot-encoding
Category = pd.get_dummies(news_data.Category)
new_cat = pd.concat([news_data, Category], axis=1)
new_cat = new_cat.drop(columns='Category')
new_cat.head(10)

# change dataframe value to numpy array
news = new_cat['Text'].values
label = new_cat[['business', 'entertainment','health','news','politics','sport']].values

# cek news
news

#cek label
label

"""**Split dataset 20% test / validasi**"""

X_train,X_test,y_train,y_test = train_test_split(news, label,test_size = 0.2,shuffle=True)

"""## **Tokenizer dan Pemodelan Sequential dengan Embedding dan LSTM**"""

vocab_size = 5000
max_len = 100
trunc_type = "post"
oov_tok = "<OOV>"

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(X_train)

word_index = tokenizer.word_index

sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_test = tokenizer.texts_to_sequences(X_test)
pad_train = pad_sequences(sequences_train, maxlen=max_len, truncating=trunc_type)
pad_test = pad_sequences(sequences_test, maxlen=max_len, truncating=trunc_type)

print(pad_test.shape)

pad_train

pad_test

# model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(6, activation='softmax')
])
model.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy',)
model.summary()

# callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.91 and logs.get('val_accuracy')>0.91):
      self.model.stop_training = True
      print("\n akurasi dari training set dan validation set telah terpenuhi > 91%!")
callbacks = myCallback()

num_epochs = 50
history = model.fit(pad_train, y_train, epochs=num_epochs,
                    validation_data=(pad_test, y_test), verbose=2, callbacks=[callbacks])

"""## **Grafik**"""

# plot of accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# plot of loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()